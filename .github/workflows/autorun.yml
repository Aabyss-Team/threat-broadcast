name: Crawl CVEs and Deploy to GitHub Pages

on:
  push:
    branches: [master]
  workflow_dispatch:
  schedule:
    - cron: '0 0 1 * *'  # 每月1号 UTC 0点运行

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  crawl-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: 3.8

      - uses: actions/cache@v3
        name: Cache pip dependencies
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: python -m pip install -r requirements.txt

      - name: Run CVE crawler
        run: |
          mkdir -p docs
          python main.py -g "${{ secrets.CRAWL_PWD }}" > docs/index.html

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload site to GitHub Pages
        uses: actions/upload-pages-artifact@v2
        with:
          path: './docs'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v3
